{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ed84ac-4e39-4677-acf5-268cc23c811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1) SETUP ==========\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- I/O (adjust paths if needed) ---\n",
    "dividend_df = pd.read_csv(\"Dividend_Dataset.csv\", parse_dates=[\"Announcement_Date\"])\n",
    "returns_df  = pd.read_csv(\"Merged_Data.csv\",    parse_dates=[\"Date\"])\n",
    "\n",
    "# Expected columns:\n",
    "# dividend_df: Announcement_Date, Ticker, [Dividend_Type, Dividend_Amount, Company_Name, ...]\n",
    "# returns_df : Date, Ticker, Stock_Returns, Market_Returns\n",
    "\n",
    "# --- Parameters (trading-day based) ---\n",
    "EST_LENGTH    = 110   # use exactly 110 clean obs\n",
    "BUFFER_LENGTH = 10    # buffer right before event (excluded from estimation)\n",
    "EVENT_K       = 5     # event window [-5, +5]\n",
    "\n",
    "# --- Safety: types, sort, dedupe events ---\n",
    "returns_df[\"Date\"] = pd.to_datetime(returns_df[\"Date\"])\n",
    "dividend_df[\"Announcement_Date\"] = pd.to_datetime(dividend_df[\"Announcement_Date\"])\n",
    "returns_df = returns_df.sort_values([\"Ticker\",\"Date\"]).reset_index(drop=True)\n",
    "dividend_df = (dividend_df\n",
    "               .sort_values([\"Ticker\",\"Announcement_Date\"])\n",
    "               .drop_duplicates(subset=[\"Ticker\",\"Announcement_Date\"])\n",
    "               .reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ac2bc3f-47e4-47a9-9186-a04fdd1c8e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated alphas/betas for 702 events (dropped in estimation: 58).\n",
      "Saved: alpha_beta_by_event.csv\n"
     ]
    }
   ],
   "source": [
    "# ========== 2) ESTIMATION ONLY (STRICT) ==========\n",
    "alpha_beta_rows = []\n",
    "drop_rows_est = []\n",
    "\n",
    "def log_drop_est(ticker, D, reason, detail=None):\n",
    "    drop_rows_est.append({\n",
    "        \"Ticker\": ticker,\n",
    "        \"Event_Date\": pd.to_datetime(D),\n",
    "        \"Reason\": reason,\n",
    "        \"Detail\": detail\n",
    "    })\n",
    "\n",
    "need = EST_LENGTH + BUFFER_LENGTH\n",
    "\n",
    "for _, ev in dividend_df[[\"Ticker\",\"Announcement_Date\"]].dropna().iterrows():\n",
    "    ticker = ev[\"Ticker\"]; D = ev[\"Announcement_Date\"]\n",
    "    firm = returns_df[returns_df[\"Ticker\"] == ticker]\n",
    "    if firm.empty:\n",
    "        log_drop_est(ticker, D, \"no_returns_for_ticker\")\n",
    "        continue\n",
    "\n",
    "    pre_all = firm[firm[\"Date\"] < D]\n",
    "    if pre_all.shape[0] < need:\n",
    "        log_drop_est(ticker, D, \"insufficient_pre_event_history\",\n",
    "                     detail=f\"Have {pre_all.shape[0]} < required {need}\")\n",
    "        continue\n",
    "\n",
    "    pre = pre_all.tail(need).reset_index(drop=True)\n",
    "    est = pre.iloc[:EST_LENGTH].copy()\n",
    "\n",
    "    Ri = est[\"Stock_Returns\"].to_numpy()\n",
    "    Rm = est[\"Market_Returns\"].to_numpy()\n",
    "\n",
    "    # STRICT: must be completely clean\n",
    "    if np.isnan(Ri).any() or np.isnan(Rm).any():\n",
    "        log_drop_est(ticker, D, \"nan_in_estimation_window\",\n",
    "                     detail=f\"NaNs -> Stock_Returns:{int(np.isnan(Ri).sum())}, Market_Returns:{int(np.isnan(Rm).sum())}\")\n",
    "        continue\n",
    "\n",
    "    var_m = np.var(Rm, ddof=1)\n",
    "    if (not np.isfinite(var_m)) or (var_m == 0):\n",
    "        log_drop_est(ticker, D, \"zero_or_nan_market_variance_in_estimation_window\",\n",
    "                     detail=f\"var_m={var_m}\")\n",
    "        continue\n",
    "\n",
    "    cov_im = np.cov(Ri, Rm, ddof=1)[0, 1]\n",
    "    beta  = cov_im / var_m\n",
    "    alpha = Ri.mean() - beta * Rm.mean()\n",
    "\n",
    "    # Optional R2\n",
    "    fitted = alpha + beta * Rm\n",
    "    ss_tot = np.sum((Ri - Ri.mean())**2)\n",
    "    ss_res = np.sum((Ri - fitted)**2)\n",
    "    R2 = 1 - ss_res / ss_tot if ss_tot > 0 else np.nan\n",
    "\n",
    "    alpha_beta_rows.append({\n",
    "        \"Ticker\": ticker,\n",
    "        \"Event_Date\": D,\n",
    "        \"Alpha\": float(alpha),\n",
    "        \"Beta\": float(beta),\n",
    "        \"R2_optional\": float(R2) if np.isfinite(R2) else np.nan,\n",
    "        \"Estimation_Obs\": EST_LENGTH\n",
    "    })\n",
    "\n",
    "alpha_beta_df = pd.DataFrame(alpha_beta_rows).sort_values([\"Ticker\",\"Event_Date\"]).reset_index(drop=True)\n",
    "drop_log_est  = pd.DataFrame(drop_rows_est).sort_values([\"Event_Date\",\"Ticker\"]).reset_index(drop=True)\n",
    "\n",
    "# --- Save αβ estimates (requested) ---\n",
    "alpha_beta_df.to_csv(\"alpha_beta_by_event.csv\", index=False)\n",
    "\n",
    "print(f\"Estimated alphas/betas for {alpha_beta_df.shape[0]} events \"\n",
    "      f\"(dropped in estimation: {drop_log_est.shape[0]}).\")\n",
    "print(\"Saved: alpha_beta_by_event.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "656591ec-9512-418c-8fce-c88153991c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Event_Date</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Beta</th>\n",
       "      <th>R2_optional</th>\n",
       "      <th>Estimation_Obs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADANIENT.NS</td>\n",
       "      <td>2016-03-09</td>\n",
       "      <td>-0.000245</td>\n",
       "      <td>1.934967</td>\n",
       "      <td>0.371993</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADANIENT.NS</td>\n",
       "      <td>2017-05-24</td>\n",
       "      <td>0.004341</td>\n",
       "      <td>1.355969</td>\n",
       "      <td>0.051619</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADANIENT.NS</td>\n",
       "      <td>2018-05-10</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>2.711392</td>\n",
       "      <td>0.334825</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADANIENT.NS</td>\n",
       "      <td>2019-05-29</td>\n",
       "      <td>-0.002520</td>\n",
       "      <td>1.776056</td>\n",
       "      <td>0.210455</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADANIENT.NS</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>0.004043</td>\n",
       "      <td>1.457345</td>\n",
       "      <td>0.363193</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>WIPRO.NS</td>\n",
       "      <td>2022-03-25</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.951547</td>\n",
       "      <td>0.367928</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>WIPRO.NS</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>-0.001342</td>\n",
       "      <td>1.042780</td>\n",
       "      <td>0.425205</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>WIPRO.NS</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>1.289917</td>\n",
       "      <td>0.316941</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>WIPRO.NS</td>\n",
       "      <td>2025-01-17</td>\n",
       "      <td>0.001834</td>\n",
       "      <td>1.184604</td>\n",
       "      <td>0.374806</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>WIPRO.NS</td>\n",
       "      <td>2025-07-17</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>1.304249</td>\n",
       "      <td>0.301366</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>702 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Ticker Event_Date     Alpha      Beta  R2_optional  Estimation_Obs\n",
       "0    ADANIENT.NS 2016-03-09 -0.000245  1.934967     0.371993             110\n",
       "1    ADANIENT.NS 2017-05-24  0.004341  1.355969     0.051619             110\n",
       "2    ADANIENT.NS 2018-05-10  0.000159  2.711392     0.334825             110\n",
       "3    ADANIENT.NS 2019-05-29 -0.002520  1.776056     0.210455             110\n",
       "4    ADANIENT.NS 2020-03-12  0.004043  1.457345     0.363193             110\n",
       "..           ...        ...       ...       ...          ...             ...\n",
       "697     WIPRO.NS 2022-03-25 -0.000004  0.951547     0.367928             110\n",
       "698     WIPRO.NS 2023-01-13 -0.001342  1.042780     0.425205             110\n",
       "699     WIPRO.NS 2024-01-12 -0.000001  1.289917     0.316941             110\n",
       "700     WIPRO.NS 2025-01-17  0.001834  1.184604     0.374806             110\n",
       "701     WIPRO.NS 2025-07-17  0.001453  1.304249     0.301366             110\n",
       "\n",
       "[702 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_beta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "377775c1-e89d-4208-a66f-24b8bb5b8084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built symmetric panel for [-5, +5] with 7183 rows across 653 events.\n",
      "\n",
      "AAR table (pooled by event day):\n",
      " Event_Day_Index  Number_of_Events  Average_Abnormal_Return  Standard_Deviation  Standard_Error  t_statistic\n",
      "              -5               653                -0.000358            0.016632        0.000651    -0.550451\n",
      "              -4               653                 0.000642            0.016596        0.000649     0.988550\n",
      "              -3               653                 0.000163            0.015608        0.000611     0.267362\n",
      "              -2               653                 0.000010            0.015567        0.000609     0.016326\n",
      "              -1               653                -0.000391            0.017445        0.000683    -0.573344\n",
      "               0               653                 0.000834            0.021052        0.000824     1.012865\n",
      "               1               653                -0.000276            0.029670        0.001161    -0.237695\n",
      "               2               653                 0.000725            0.018007        0.000705     1.028892\n",
      "               3               653                 0.001232            0.016063        0.000629     1.960473\n",
      "               4               653                -0.000205            0.015783        0.000618    -0.331171\n",
      "               5               653                -0.000718            0.014913        0.000584    -1.230240\n",
      "\n",
      "CAAR summary for [-5, +5] window:\n",
      " Number_of_Events  CAAR_from_AAR_Sum  CAAR_from_Mean_of_CARs  Standard_Deviation_of_CARs  Standard_Error_of_CARs  t_statistic_for_Mean_CAR\n",
      "              653           0.001659                0.001659                    0.063451                0.002483                  0.668107\n",
      "\n",
      "MERGING CAR WITH DIVIDEND DATA\n",
      "========================================\n",
      "CAR events: 653\n",
      "Merged events: 657\n",
      "Events with dividend data: 657\n",
      "Events without dividend data: 0\n",
      "Saved: merged_dividend_car_data.csv\n"
     ]
    }
   ],
   "source": [
    "# ========== 3) EVENT WINDOW + AAR/CAAR (same format as before; no significance flags) ==========\n",
    "\n",
    "panel_rows = []\n",
    "\n",
    "for _, row in alpha_beta_df.iterrows():\n",
    "    ticker = row[\"Ticker\"]\n",
    "    D      = row[\"Event_Date\"]\n",
    "    alpha  = row[\"Alpha\"]\n",
    "    beta   = row[\"Beta\"]\n",
    "\n",
    "    firm = returns_df[returns_df[\"Ticker\"] == ticker]\n",
    "    if firm.empty:\n",
    "        continue\n",
    "\n",
    "    # K trading days before (strictly < D) and 0..+K (>= D)\n",
    "    preK  = firm[firm[\"Date\"] < D].tail(EVENT_K)\n",
    "    postK = firm[firm[\"Date\"] >= D].head(EVENT_K + 1)\n",
    "\n",
    "    # Require exact coverage\n",
    "    if (len(preK) < EVENT_K) or (len(postK) < (EVENT_K + 1)):\n",
    "        continue\n",
    "\n",
    "    preK  = preK.sort_values(\"Date\").assign(event_day_index=np.arange(-EVENT_K, 0, 1))\n",
    "    postK = postK.sort_values(\"Date\").assign(event_day_index=np.arange(0, EVENT_K + 1, 1))\n",
    "    window = pd.concat([preK, postK], ignore_index=True)\n",
    "\n",
    "    # Expected & abnormal returns (Market Model)\n",
    "    expected = alpha + beta * window[\"Market_Returns\"].to_numpy()\n",
    "    AR       = window[\"Stock_Returns\"].to_numpy() - expected\n",
    "\n",
    "    panel_rows.append(pd.DataFrame({\n",
    "        \"Ticker\": ticker,\n",
    "        \"event_date\": D,\n",
    "        \"trading_date\": window[\"Date\"].to_numpy(),\n",
    "        \"event_day_index\": window[\"event_day_index\"].to_numpy(),\n",
    "        \"Market_Return\": window[\"Market_Returns\"].to_numpy(),\n",
    "        \"Stock_Return\": window[\"Stock_Returns\"].to_numpy(),\n",
    "        \"Expected_Return\": expected,\n",
    "        \"Abnormal_Return\": AR\n",
    "    }))\n",
    "\n",
    "sym_panel = (\n",
    "    pd.concat(panel_rows, ignore_index=True)\n",
    "    if panel_rows else\n",
    "    pd.DataFrame(columns=[\n",
    "        \"Ticker\",\"event_date\",\"trading_date\",\"event_day_index\",\n",
    "        \"Market_Return\",\"Stock_Return\",\"Expected_Return\",\"Abnormal_Return\"\n",
    "    ])\n",
    ")\n",
    "\n",
    "# ---------- AAR (per event day) ----------\n",
    "if sym_panel.empty:\n",
    "    aar_table = pd.DataFrame(columns=[\n",
    "        \"Event_Day_Index\",\"Number_of_Events\",\"Average_Abnormal_Return\",\n",
    "        \"Standard_Deviation\",\"Standard_Error\",\"t_statistic\"\n",
    "    ])\n",
    "else:\n",
    "    aar = (\n",
    "        sym_panel.groupby(\"event_day_index\")[\"Abnormal_Return\"]\n",
    "                 .agg(Number_of_Events=\"count\",\n",
    "                      Average_Abnormal_Return=\"mean\",\n",
    "                      Standard_Deviation=lambda x: x.std(ddof=1))\n",
    "                 .reset_index()\n",
    "                 .sort_values(\"event_day_index\")\n",
    "    )\n",
    "    aar[\"Standard_Error\"] = aar[\"Standard_Deviation\"] / np.sqrt(aar[\"Number_of_Events\"])\n",
    "    aar[\"t_statistic\"]    = aar[\"Average_Abnormal_Return\"] / aar[\"Standard_Error\"]\n",
    "    aar_table = aar.rename(columns={\"event_day_index\": \"Event_Day_Index\"})\n",
    "\n",
    "# ---------- CAAR over [-K, +K] ----------\n",
    "if sym_panel.empty:\n",
    "    caar_summary = pd.DataFrame([{\n",
    "        \"Number_of_Events\": 0,\n",
    "        \"CAAR_from_AAR_Sum\": np.nan,\n",
    "        \"CAAR_from_Mean_of_CARs\": np.nan,\n",
    "        \"Standard_Deviation_of_CARs\": np.nan,\n",
    "        \"Standard_Error_of_CARs\": np.nan,\n",
    "        \"t_statistic_for_Mean_CAR\": np.nan\n",
    "    }])\n",
    "    car_by_event = pd.DataFrame(columns=[\"Ticker\",\"event_date\",\"CAR\"])\n",
    "else:\n",
    "    caar_from_aar = float(aar_table[\"Average_Abnormal_Return\"].sum())\n",
    "    car_by_event = (\n",
    "        sym_panel.groupby([\"Ticker\",\"event_date\"], as_index=False)\n",
    "                 .agg(CAR=(\"Abnormal_Return\",\"sum\"))\n",
    "                 .sort_values([\"Ticker\",\"event_date\"])\n",
    "    )\n",
    "    N_ev   = car_by_event.shape[0]\n",
    "    caar_mean = float(car_by_event[\"CAR\"].mean()) if N_ev > 0 else np.nan\n",
    "    caar_sd   = float(car_by_event[\"CAR\"].std(ddof=1)) if N_ev > 1 else np.nan\n",
    "    caar_se   = (caar_sd / np.sqrt(N_ev)) if N_ev > 1 else np.nan\n",
    "    caar_t    = (caar_mean / caar_se) if (isinstance(caar_se, float) and caar_se > 0) else np.nan\n",
    "\n",
    "    caar_summary = pd.DataFrame([{\n",
    "        \"Number_of_Events\": N_ev,\n",
    "        \"CAAR_from_AAR_Sum\": caar_from_aar,\n",
    "        \"CAAR_from_Mean_of_CARs\": caar_mean,\n",
    "        \"Standard_Deviation_of_CARs\": caar_sd,\n",
    "        \"Standard_Error_of_CARs\": caar_se,\n",
    "        \"t_statistic_for_Mean_CAR\": caar_t\n",
    "    }])\n",
    "\n",
    "# ---------- OUTPUT (same style as your earlier printout) ----------\n",
    "n_kept = sym_panel[[\"Ticker\",\"event_date\"]].drop_duplicates().shape[0] if not sym_panel.empty else 0\n",
    "print(f\"Built symmetric panel for [-{EVENT_K}, +{EVENT_K}] with {len(sym_panel)} rows across {n_kept} events.\")\n",
    "\n",
    "print(\"\\nAAR table (pooled by event day):\")\n",
    "print(aar_table.to_string(index=False))\n",
    "\n",
    "print(\"\\nCAAR summary for [-{0}, +{0}] window:\".format(EVENT_K))\n",
    "print(caar_summary.to_string(index=False))\n",
    "\n",
    "# ---------- SAVE EXTRA CSVs (but don't print contents) ----------\n",
    "#sym_panel.to_csv(f\"AR_panel_[-{EVENT_K},+{EVENT_K}].csv\", index=False)\n",
    "#aar_table.to_csv(f\"AAR_table_[-{EVENT_K},+{EVENT_K}].csv\", index=False)\n",
    "#caar_summary.to_csv(f\"CAAR_summary_[-{EVENT_K},+{EVENT_K}].csv\", index=False)\n",
    "car_by_event.to_csv(f\"CAR_by_event.csv\", index=False)\n",
    "\n",
    "# ========== MERGE CAR WITH DIVIDEND DATA ==========\n",
    "print(\"\\nMERGING CAR WITH DIVIDEND DATA\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Load the dividend dataset \n",
    "dividend_df = pd.read_csv(\"Dividend_Dataset.csv\", parse_dates=[\"Announcement_Date\"])\n",
    "car_df = pd.read_csv(\"CAR_by_event.csv\", parse_dates=[\"event_date\"])\n",
    "\n",
    "# Rename and merge\n",
    "car_df = car_df.rename(columns={'event_date': 'Announcement_Date'})\n",
    "merged_df = pd.merge(\n",
    "    car_df,\n",
    "    dividend_df[['Announcement_Date', 'Ticker', 'Dividend_Amount', 'Dividend_Type']],\n",
    "    on=['Announcement_Date', 'Ticker'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Rename Dividend_Amount to DPS\n",
    "merged_df = merged_df.rename(columns={'Dividend_Amount': 'DPS'})\n",
    "\n",
    "print(f\"CAR events: {len(car_df)}\")\n",
    "print(f\"Merged events: {len(merged_df)}\")\n",
    "print(f\"Events with dividend data: {merged_df['DPS'].notnull().sum()}\")\n",
    "print(f\"Events without dividend data: {merged_df['DPS'].isnull().sum()}\")\n",
    "\n",
    "# Save the merged dataset\n",
    "merged_df.to_csv('merged_dividend_car_data.csv', index=False)\n",
    "print(\"Saved: merged_dividend_car_data.csv\")\n",
    "\n",
    "# ========== NOW CONTINUE WITH PRICE MERGE =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7042d711-54fe-4275-8ca4-3a9c960b57dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682ae3e6-205b-44d8-983e-70238aa709e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbddb082-00e6-44c8-97e0-3f5b154d3fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268eaac9-eb73-4f45-b7c9-9da319bb9e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "27db2685-e6b0-4079-acc5-7aba88a69358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Ticker Announcement_Date       CAR  DPS Dividend_Type\n",
      "0  ADANIENT.NS        2016-03-09 -0.016032  0.4       Interim\n",
      "1  ADANIENT.NS        2017-05-24 -0.108284  0.4         Final\n",
      "2  ADANIENT.NS        2018-05-10 -0.039030  0.4         Final\n",
      "3  ADANIENT.NS        2019-05-29 -0.000906  0.4         Final\n",
      "4  ADANIENT.NS        2020-03-12 -0.135723  1.0       Interim\n",
      "\n",
      "Merged dataset shape: (657, 5)\n",
      "\n",
      "Columns in merged dataset: ['Ticker', 'Announcement_Date', 'CAR', 'DPS', 'Dividend_Type']\n",
      "\n",
      "Missing values in each column:\n",
      "Ticker               0\n",
      "Announcement_Date    0\n",
      "CAR                  0\n",
      "DPS                  0\n",
      "Dividend_Type        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have both datasets loaded as DataFrames\n",
    "dividend_df = pd.read_csv('Dividend_Dataset.csv')  # or your file path\n",
    "car_df = pd.read_csv('CAR_by_event.csv')           # or your file path\n",
    "\n",
    "# Convert date columns to datetime to ensure proper matching\n",
    "dividend_df['Announcement_Date'] = pd.to_datetime(dividend_df['Announcement_Date'])\n",
    "car_df['event_date'] = pd.to_datetime(car_df['event_date'])\n",
    "\n",
    "# First, rename event_date to Announcement_Date in the car_df\n",
    "car_df = car_df.rename(columns={'event_date': 'Announcement_Date'})\n",
    "\n",
    "# Now merge the datasets\n",
    "merged_df = pd.merge(\n",
    "    car_df,\n",
    "    dividend_df[['Announcement_Date', 'Ticker', 'Dividend_Amount', 'Dividend_Type']],\n",
    "    on=['Announcement_Date', 'Ticker'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Rename Dividend_Amount to DPS\n",
    "merged_df = merged_df.rename(columns={'Dividend_Amount': 'DPS'})\n",
    "\n",
    "# Display the merged dataframe\n",
    "print(merged_df.head())\n",
    "print(f\"\\nMerged dataset shape: {merged_df.shape}\")\n",
    "print(f\"\\nColumns in merged dataset: {merged_df.columns.tolist()}\")\n",
    "\n",
    "# Check for any missing values in the merged data\n",
    "print(f\"\\nMissing values in each column:\")\n",
    "print(merged_df.isnull().sum())\n",
    "\n",
    "# If you want to save the merged dataset\n",
    "merged_df.to_csv('merged_dividend_car_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb33068-de3a-47e9-9876-2a4a146874a3",
   "metadata": {},
   "source": [
    "### Dividend_Yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26eaa437-1299-4d1a-a713-42192ac11367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DataFrame Shape: (657, 8)\n",
      "\n",
      "Columns in final dataset:\n",
      "['Ticker', 'Announcement_Date', 'CAR', 'DPS', 'Dividend_Type', 'Event_Date_AdjClose', 'Prior_Day_AdjClose', 'Dividend_Yield']\n",
      "\n",
      "New columns added: Event_Date_AdjClose, Prior_Day_AdjClose, Dividend_Yield\n",
      "\n",
      "First few rows:\n",
      "        Ticker Announcement_Date       CAR  DPS Dividend_Type  \\\n",
      "0  ADANIENT.NS        2016-03-09 -0.016032  0.4       Interim   \n",
      "1  ADANIENT.NS        2017-05-24 -0.108284  0.4         Final   \n",
      "2  ADANIENT.NS        2018-05-10 -0.039030  0.4         Final   \n",
      "3  ADANIENT.NS        2019-05-29 -0.000906  0.4         Final   \n",
      "4  ADANIENT.NS        2020-03-12 -0.135723  1.0       Interim   \n",
      "\n",
      "   Event_Date_AdjClose  Prior_Day_AdjClose  Dividend_Yield  \n",
      "0            35.040146           33.800018        0.011834  \n",
      "1            60.596619           60.356586        0.006627  \n",
      "2            79.972527           81.487999        0.004909  \n",
      "3           153.172226          155.640350        0.002570  \n",
      "4           160.570099          193.842728        0.005159  \n",
      "\n",
      "Missing values in new columns:\n",
      "Event_Date_AdjClose    3\n",
      "Prior_Day_AdjClose     0\n",
      "Dividend_Yield         0\n",
      "dtype: int64\n",
      "\n",
      "Sample verification (showing announcement date and most recent trading date):\n",
      "Ticker: ADANIENT.NS, Announcement: 2016-03-09, Most Recent Trading: 2016-03-08\n",
      "Ticker: ADANIENT.NS, Announcement: 2017-05-24, Most Recent Trading: 2017-05-23\n",
      "Ticker: ADANIENT.NS, Announcement: 2018-05-10, Most Recent Trading: 2018-05-09\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the stock prices dataset\n",
    "prices_df = pd.read_csv('nifty50_stock_prices.csv')\n",
    "\n",
    "# Convert date columns to datetime format\n",
    "prices_df['Date'] = pd.to_datetime(prices_df['Date'])\n",
    "merged_df['Announcement_Date'] = pd.to_datetime(merged_df['Announcement_Date'])\n",
    "\n",
    "# Create a copy to avoid modifying the original during operations\n",
    "result_df = merged_df.copy()\n",
    "\n",
    "# Get event date's AdjClose price\n",
    "event_prices = prices_df[['Date', 'Ticker', 'AdjClose']].copy()\n",
    "event_prices = event_prices.rename(columns={'AdjClose': 'Event_Date_AdjClose'})\n",
    "\n",
    "result_df = pd.merge(\n",
    "    result_df,\n",
    "    event_prices,\n",
    "    left_on=['Announcement_Date', 'Ticker'],\n",
    "    right_on=['Date', 'Ticker'],\n",
    "    how='left'\n",
    ").drop('Date', axis=1)\n",
    "\n",
    "# Create a dictionary of all available trading dates for each ticker\n",
    "available_dates = {}\n",
    "for ticker in prices_df['Ticker'].unique():\n",
    "    ticker_dates = set(prices_df[prices_df['Ticker'] == ticker]['Date'])\n",
    "    available_dates[ticker] = sorted(ticker_dates)\n",
    "\n",
    "def get_most_recent_trading_price(ticker, announcement_date):\n",
    "    \"\"\"Get the most recent trading day price before announcement date\"\"\"\n",
    "    if ticker not in available_dates:\n",
    "        return None\n",
    "    \n",
    "    # Get all trading dates that are BEFORE the announcement date\n",
    "    prior_dates = [date for date in available_dates[ticker] if date < announcement_date]\n",
    "    \n",
    "    if not prior_dates:\n",
    "        return None\n",
    "    \n",
    "    # Get the most recent trading date\n",
    "    most_recent_date = max(prior_dates)\n",
    "    \n",
    "    # Get the price for that date\n",
    "    return prices_df[(prices_df['Ticker'] == ticker) & \n",
    "                    (prices_df['Date'] == most_recent_date)]['AdjClose'].values[0]\n",
    "\n",
    "# Get previous trading day's AdjClose price\n",
    "result_df['Prior_Day_AdjClose'] = result_df.apply(\n",
    "    lambda row: get_most_recent_trading_price(row['Ticker'], row['Announcement_Date']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Calculate Dividend Yield (DPS / Prior Trading Day AdjClose Price)\n",
    "result_df['Dividend_Yield'] = result_df['DPS'] / result_df['Prior_Day_AdjClose']\n",
    "\n",
    "# Display the results\n",
    "print(\"Final DataFrame Shape:\", result_df.shape)\n",
    "print(\"\\nColumns in final dataset:\")\n",
    "print(result_df.columns.tolist())\n",
    "print(f\"\\nNew columns added: Event_Date_AdjClose, Prior_Day_AdjClose, Dividend_Yield\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(result_df.head())\n",
    "\n",
    "# Check for any missing values\n",
    "print(f\"\\nMissing values in new columns:\")\n",
    "print(result_df[['Event_Date_AdjClose', 'Prior_Day_AdjClose', 'Dividend_Yield']].isnull().sum())\n",
    "\n",
    "# Show some examples to verify the logic\n",
    "print(\"\\nSample verification (showing announcement date and most recent trading date):\")\n",
    "sample = result_df.head(3).copy()\n",
    "for _, row in sample.iterrows():\n",
    "    ticker = row['Ticker']\n",
    "    ann_date = row['Announcement_Date']\n",
    "    recent_dates = [d for d in available_dates[ticker] if d < ann_date]\n",
    "    most_recent = max(recent_dates) if recent_dates else None\n",
    "    print(f\"Ticker: {ticker}, Announcement: {ann_date.date()}, Most Recent Trading: {most_recent.date() if most_recent else 'None'}\")\n",
    "merged_df.to_csv('merged_dividend_car_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04093a10-ae29-49d8-88bb-843914572109",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('dividend_yield_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae98c83-a87f-4233-9d43-ab4a758434ef",
   "metadata": {},
   "source": [
    "### Dividend_Change_Pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3427d92-7826-4a9d-b441-30d0d64ee366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset created!\n",
      "Columns available:\n",
      "['Ticker', 'Announcement_Date', 'CAR', 'DPS', 'Dividend_Type', 'Event_Date_AdjClose', 'Prior_Day_AdjClose', 'Dividend_Yield', 'DPS_Change', 'DPS_Pct_Change', 'Dividend_Direction_Text', 'Dividend_Direction_Num']\n",
      "\n",
      "DPS_Change statistics:\n",
      "count    577.000000\n",
      "mean       1.080364\n",
      "std       18.244433\n",
      "min     -133.000000\n",
      "25%        0.000000\n",
      "50%        0.250000\n",
      "75%        2.400000\n",
      "max      157.000000\n",
      "Name: DPS_Change, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate dividend change for same dividend type\n",
    "result_df['DPS_Change'] = result_df.groupby(['Ticker', 'Dividend_Type'])['DPS'].diff()\n",
    "\n",
    "# Save to CSV\n",
    "result_df.to_csv('dividend_yield_analysis_final.csv', index=False)\n",
    "\n",
    "print(\"Final dataset created!\")\n",
    "print(\"Columns available:\")\n",
    "print(result_df.columns.tolist())\n",
    "print(f\"\\nDPS_Change statistics:\")\n",
    "print(result_df['DPS_Change'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69fd831a-767e-4372-9c74-5b03cfeaadbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete dataset created!\n",
      "Total rows: 657\n",
      "\n",
      "Columns available:\n",
      "['Ticker', 'Announcement_Date', 'CAR', 'DPS', 'Dividend_Type', 'Event_Date_AdjClose', 'Prior_Day_AdjClose', 'Dividend_Yield', 'DPS_Change', 'DPS_Pct_Change', 'Dividend_Direction_Text', 'Dividend_Direction_Num']\n",
      "\n",
      "DPS_Change statistics:\n",
      "count    577.000000\n",
      "mean       1.080364\n",
      "std       18.244433\n",
      "min     -133.000000\n",
      "25%        0.000000\n",
      "50%        0.250000\n",
      "75%        2.400000\n",
      "max      157.000000\n",
      "Name: DPS_Change, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate dividend change for same dividend type\n",
    "result_df['DPS_Change'] = result_df.groupby(['Ticker', 'Dividend_Type'])['DPS'].diff()\n",
    "\n",
    "# Calculate market percentage change (assuming you have a 'Price' column)\n",
    "result_df['DPS_Pct_Change'] = result_df.groupby('Ticker')['DPS'].pct_change()\n",
    "\n",
    "# Save dataset without direction columns\n",
    "result_df.to_csv('dividend_analysis_complete.csv', index=False)\n",
    "\n",
    "print(\"Complete dataset created!\")\n",
    "print(f\"Total rows: {len(result_df)}\")\n",
    "print(\"\\nColumns available:\")\n",
    "print(result_df.columns.tolist())\n",
    "print(f\"\\nDPS_Change statistics:\")\n",
    "print(result_df['DPS_Change'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88db3f6-d87e-41ea-bdbd-15cf262ee33b",
   "metadata": {},
   "source": [
    "# Calculate dividend change for same dividend type\n",
    "result_df['DPS_Change'] = result_df.groupby(['Ticker', 'Dividend_Type'])['DPS'].diff()\n",
    "\n",
    "# Create both text and numeric direction columns\n",
    "result_df['Dividend_Direction_Text'] = result_df['DPS_Change'].apply(\n",
    "    lambda x: 'Increase' if x > 0 else ('Decrease' if x < 0 else 'No Change') if pd.notnull(x) else None\n",
    ")\n",
    "\n",
    "result_df['Dividend_Direction_Num'] = result_df['DPS_Change'].apply(\n",
    "    lambda x: 1 if x > 0 else (-1 if x < 0 else 0) if pd.notnull(x) else None\n",
    ")\n",
    "\n",
    "# Drop rows where direction is None (first events in dataset)\n",
    "div_change_df = result_df.dropna(subset=['Dividend_Direction_Num']).copy()\n",
    "\n",
    "# Save dataset with both columns\n",
    "div_change_df.to_csv('dividend_analysis_complete.csv', index=False)\n",
    "\n",
    "print(\"Complete dataset created!\")\n",
    "print(f\"Original rows: {len(result_df)}\")\n",
    "print(f\"Rows after dropping first events: {len(regression_df)}\")\n",
    "print(\"\\nDividend direction distribution:\")\n",
    "print(regression_df['Dividend_Direction_Text'].value_counts())\n",
    "print(regression_df['Dividend_Direction_Num'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "50269778-edbc-4dd4-a10e-73013efcf81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "div_change_df.to_csv('dividend_analysis_complete.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c065b85-1f31-4cd3-9afd-591fc49cd0e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'regression_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m columns_to_remove \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROA_x\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROA_y\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMarket_Cap_Cr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMarket_Cap_Cr_x\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMarket_Cap_Cr_y\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns_to_remove:\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m regression_df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m      5\u001b[0m         regression_df \u001b[38;5;241m=\u001b[39m regression_df\u001b[38;5;241m.\u001b[39mdrop(col, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Convert ROA and Market Cap data from wide to long format USING TICKER\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'regression_df' is not defined"
     ]
    }
   ],
   "source": [
    "# First, remove any existing ROA and Market Cap columns to avoid conflicts\n",
    "columns_to_remove = ['ROA', 'ROA_x', 'ROA_y', 'Market_Cap_Cr', 'Market_Cap_Cr_x', 'Market_Cap_Cr_y']\n",
    "for col in columns_to_remove:\n",
    "    if col in regression_df.columns:\n",
    "        regression_df = regression_df.drop(col, axis=1)\n",
    "\n",
    "# Convert ROA and Market Cap data from wide to long format USING TICKER\n",
    "roa_long = pd.melt(roa_df, id_vars=['Ticker'], value_vars=year_columns, var_name='FY_Year', value_name='ROA')\n",
    "roa_long['FY_Year'] = roa_long['FY_Year'].astype(int)\n",
    "\n",
    "mcap_long = pd.melt(mcap_df, id_vars=['Ticker'], value_vars=year_columns, var_name='FY_Year', value_name='Market_Cap_Cr')\n",
    "mcap_long['FY_Year'] = mcap_long['FY_Year'].astype(int)\n",
    "\n",
    "# Create relevant FY column in main dataset\n",
    "def get_relevant_fy(announcement_date):\n",
    "    year = announcement_date.year\n",
    "    month = announcement_date.month\n",
    "    return year if month >= 4 else year - 1\n",
    "\n",
    "regression_df['Relevant_FY_Year'] = regression_df['Announcement_Date'].apply(get_relevant_fy)\n",
    "\n",
    "# Merge ROA and Market Cap using TICKER (not Company_Name)\n",
    "# First merge with ROA\n",
    "regression_df = pd.merge(\n",
    "    regression_df,\n",
    "    roa_long,\n",
    "    left_on=['Ticker', 'Relevant_FY_Year'],\n",
    "    right_on=['Ticker', 'FY_Year'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Then merge with Market Cap\n",
    "regression_df = pd.merge(\n",
    "    regression_df,\n",
    "    mcap_long,\n",
    "    left_on=['Ticker', 'Relevant_FY_Year'],\n",
    "    right_on=['Ticker', 'FY_Year'],\n",
    "    how='left',\n",
    "    suffixes=('', '_mcap')\n",
    ")\n",
    "\n",
    "# Clean up extra columns - check what columns actually exist before dropping\n",
    "columns_to_drop = []\n",
    "for col in ['FY_Year', 'FY_Year_roa', 'FY_Year_mcap']:\n",
    "    if col in regression_df.columns:\n",
    "        columns_to_drop.append(col)\n",
    "\n",
    "if columns_to_drop:\n",
    "    regression_df = regression_df.drop(columns_to_drop, axis=1)\n",
    "\n",
    "# Alternative: drop columns that end with these patterns\n",
    "# regression_df = regression_df.loc[:, ~regression_df.columns.str.endswith(('_x', '_y', '_roa', '_mcap'))]\n",
    "\n",
    "# Save final dataset\n",
    "regression_df.to_csv('complete_dividend_analysis_dataset.csv', index=False)\n",
    "\n",
    "print(\"Merging completed successfully!\")\n",
    "print(f\"Final dataset shape: {regression_df.shape}\")\n",
    "print(f\"Columns: {list(regression_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2b1bd318-f556-427f-ac62-4027d578ccef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate columns removed!\n",
      "Final columns: ['Announcement_Date', 'Ticker', 'Dividend_Type', 'DPS', 'DPS_Change', 'DPS_Pct_Change', 'Dividend_Direction_Text', 'Dividend_Direction_Num', 'Event_Date_AdjClose', 'Prior_Day_AdjClose', 'Dividend_Yield', 'CAR']\n"
     ]
    }
   ],
   "source": [
    "# 1. Drop duplicate columns\n",
    "if 'Dividend_Direction' in regression_df.columns:\n",
    "    regression_df = regression_df.drop('Dividend_Direction', axis=1)\n",
    "\n",
    "# 2. Drop any FY_Year_x, FY_Year_y columns\n",
    "fy_columns_to_drop = ['FY_Year_x', 'FY_Year_y', 'FY_Year_mcap', 'ROA_FY_Year', 'MCap_FY_Year',  'Company_Name_x', 'Company_Name_y','Market_Cap_Cr_mcap', 'Log_Market_Cap_Cr','Market_Pct_Change']\n",
    "for col in fy_columns_to_drop:\n",
    "    if col in regression_df.columns:\n",
    "        regression_df = regression_df.drop(col, axis=1)\n",
    "\n",
    "# 3. Rename Relevant_FY_Year to FY_Year\n",
    "if 'Relevant_FY_Year' in regression_df.columns:\n",
    "    regression_df = regression_df.rename(columns={'Relevant_FY_Year': 'FY_Year'})\n",
    "\n",
    "# 4. Rearrange columns (simplified version)\n",
    "essential_columns = [\n",
    "    'Announcement_Date', 'Company_Name', 'Ticker', 'FY_Year', 'Dividend_Type',\n",
    "    'DPS', 'DPS_Change', 'DPS_Pct_Change', 'Dividend_Direction_Text', 'Dividend_Direction_Num',\n",
    "    'Event_Date_AdjClose', 'Prior_Day_AdjClose', 'Dividend_Yield',\n",
    "    'ROA', 'Market_Cap_Cr', 'CAR'\n",
    "]\n",
    "\n",
    "# Keep only columns that exist\n",
    "existing_columns = [col for col in essential_columns if col in regression_df.columns]\n",
    "other_columns = [col for col in regression_df.columns if col not in existing_columns]\n",
    "regression_df = regression_df[existing_columns + other_columns]\n",
    "\n",
    "# 5. Save clean dataset\n",
    "regression_df.to_csv('REGRESSION_DATASET.csv', index=False)\n",
    "\n",
    "print(\"Duplicate columns removed!\")\n",
    "print(\"Final columns:\", regression_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8179cd2b-69a5-484c-b876-aad5b3dfb0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types after cleaning:\n",
      "CAR                  float64\n",
      "Dividend_Yield       float64\n",
      "DPS_Pct_Change       float64\n",
      "Market_Cap_Cr        float64\n",
      "ROA                  float64\n",
      "Dividend_Type_num      int64\n",
      "Ticker                object\n",
      "dtype: object\n",
      "\n",
      "Final dataset shape: (577, 7)\n",
      "\n",
      "==================================================\n",
      "REGRESSION RESULTS\n",
      "==================================================\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    CAR   R-squared:                       0.021\n",
      "Model:                            OLS   Adj. R-squared:                  0.012\n",
      "Method:                 Least Squares   F-statistic:                     4.376\n",
      "Date:                Thu, 04 Sep 2025   Prob (F-statistic):            0.00433\n",
      "Time:                        08:01:44   Log-Likelihood:                 783.18\n",
      "No. Observations:                 577   AIC:                            -1554.\n",
      "Df Residuals:                     571   BIC:                            -1528.\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:              cluster                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const                 0.0075      0.007      1.115      0.265      -0.006       0.021\n",
      "Dividend_Yield        0.1675      0.050      3.330      0.001       0.069       0.266\n",
      "DPS_Pct_Change       -0.0004      0.002     -0.229      0.819      -0.003       0.003\n",
      "Market_Cap_Cr      2.362e-09   1.59e-09      1.481      0.139   -7.63e-10    5.49e-09\n",
      "ROA                  -0.0002      0.000     -0.770      0.441      -0.001       0.000\n",
      "Dividend_Type_num    -0.0119      0.007     -1.789      0.074      -0.025       0.001\n",
      "==============================================================================\n",
      "Omnibus:                       42.043   Durbin-Watson:                   1.988\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              113.777\n",
      "Skew:                           0.339   Prob(JB):                     1.97e-25\n",
      "Kurtosis:                       5.067   Cond. No.                     3.72e+07\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are robust to cluster correlation (cluster)\n",
      "[2] The condition number is large, 3.72e+07. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adit Chaudhari\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 5, but rank is 4\n",
      "  warnings.warn('covariance of constraints does not have full '\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# ===== 1) Load data from CSV and define columns =====\n",
    "df = pd.read_csv('REGRESSION_DATASET_updated.csv')  # Change filename if different\n",
    "\n",
    "needed_cols = ['CAR','Dividend_Yield','DPS_Pct_Change','Market_Cap_Cr','ROA','Dividend_Type','Ticker']\n",
    "missing = [c for c in needed_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "# ===== 2) Clean / coerce numerics (handles %, commas, and ( ) negatives) =====\n",
    "def to_numeric_clean(s):\n",
    "    # Works for both numeric and object series\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        return pd.to_numeric(s, errors='coerce')\n",
    "    s = s.astype(str).str.strip()\n",
    "    # detect percentage and negative-in-parentheses\n",
    "    has_pct = s.str.contains('%', na=False)\n",
    "    neg_paren = s.str.match(r'^\\(.*\\)$', na=False)\n",
    "\n",
    "    # remove commas, %, parentheses\n",
    "    cleaned = s.str.replace(',', '', regex=False)\\\n",
    "               .str.replace('%', '', regex=False)\\\n",
    "               .str.replace('(', '', regex=False)\\\n",
    "               .str.replace(')', '', regex=False)\n",
    "\n",
    "    vals = pd.to_numeric(cleaned, errors='coerce')\n",
    "    # apply signs/scale\n",
    "    vals[neg_paren] = -vals[neg_paren]\n",
    "    vals[has_pct] = vals[has_pct] / 100.0\n",
    "    return vals\n",
    "\n",
    "num_cols = ['CAR','Dividend_Yield','DPS_Change','Market_Cap_Cr','ROA']\n",
    "for c in num_cols:\n",
    "    df[c] = to_numeric_clean(df[c])\n",
    "\n",
    "# ===== 3) Make Dividend_Type numeric dummy (0/1) if not already numeric =====\n",
    "# Assumes common labels like 'Final'/'Interim' or similar; adjust mapping if needed.\n",
    "if not pd.api.types.is_numeric_dtype(df['Dividend_Type']):\n",
    "    type_map = {\n",
    "        'final': 1, 'f': 1,\n",
    "        'interim': 0, 'i': 0\n",
    "    }\n",
    "    # safe lowercasing for mapping\n",
    "    dt_lower = df['Dividend_Type'].astype(str).str.strip().str.lower()\n",
    "    # start with NaN then fill from mapping\n",
    "    dt_num = pd.Series(np.nan, index=df.index)\n",
    "    dt_num = dt_lower.map(type_map)\n",
    "    # if some are already numeric-like strings, coerce them\n",
    "    fallback_numeric = pd.to_numeric(df['Dividend_Type'], errors='coerce')\n",
    "    dt_num = dt_num.fillna(fallback_numeric)\n",
    "    df['Dividend_Type_num'] = dt_num\n",
    "else:\n",
    "    df['Dividend_Type_num'] = pd.to_numeric(df['Dividend_Type'], errors='coerce')\n",
    "\n",
    "# ===== 4) Drop rows with missing essentials or non-finite =====\n",
    "keep_cols = ['CAR','Dividend_Yield','DPS_Pct_Change','Market_Cap_Cr','ROA','Dividend_Type_num','Ticker']\n",
    "df = df[keep_cols].copy()\n",
    "\n",
    "# Remove inf/-inf\n",
    "for c in keep_cols[:-1]:  # exclude Ticker\n",
    "    df = df[np.isfinite(df[c])]\n",
    "\n",
    "# Drop NaNs\n",
    "df = df.dropna(subset=keep_cols)\n",
    "\n",
    "# Ensure Ticker is present and usable for clustering\n",
    "df['Ticker'] = df['Ticker'].astype(str)\n",
    "\n",
    "# Sanity check: types\n",
    "print(\"Data types after cleaning:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nFinal dataset shape: {df.shape}\")\n",
    "\n",
    "# ===== 5) Build regression matrices =====\n",
    "y = df['CAR']\n",
    "X = df[['Dividend_Yield','DPS_Pct_Change','Market_Cap_Cr','ROA','Dividend_Type_num']]\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# ===== 6) Fit OLS with clustered SEs by Ticker (fallback to HC3 if only one firm) =====\n",
    "groups = df['Ticker']\n",
    "if groups.nunique() > 1:\n",
    "    model = sm.OLS(y, X).fit(cov_type='cluster', cov_kwds={'groups': groups})\n",
    "else:\n",
    "    model = sm.OLS(y, X).fit(cov_type='HC3')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"REGRESSION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b4e5e6-1db1-4ecd-ad59-0284accd0383",
   "metadata": {},
   "source": [
    "### Only Features in this CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ee41e630-99b0-492e-a38c-29f06088a1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Company</th>\n",
       "      <th>Event_Date</th>\n",
       "      <th>Dividend_Yield</th>\n",
       "      <th>Dividend_Change_Pct</th>\n",
       "      <th>Momentum_20d</th>\n",
       "      <th>Volatility_30d</th>\n",
       "      <th>Beta</th>\n",
       "      <th>DivType_Interim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADANIENT.NS</td>\n",
       "      <td>Adani Enterprises Limited</td>\n",
       "      <td>2021-05-05</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147261</td>\n",
       "      <td>0.043471</td>\n",
       "      <td>1.326688</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADANIENT.NS</td>\n",
       "      <td>Adani Enterprises Limited</td>\n",
       "      <td>2022-05-04</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154828</td>\n",
       "      <td>0.020791</td>\n",
       "      <td>1.501592</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADANIENT.NS</td>\n",
       "      <td>Adani Enterprises Limited</td>\n",
       "      <td>2023-05-04</td>\n",
       "      <td>0.000653</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.058887</td>\n",
       "      <td>0.028315</td>\n",
       "      <td>3.458845</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADANIENT.NS</td>\n",
       "      <td>Adani Enterprises Limited</td>\n",
       "      <td>2024-05-02</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>-0.043985</td>\n",
       "      <td>0.016967</td>\n",
       "      <td>1.732479</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADANIENT.NS</td>\n",
       "      <td>Adani Enterprises Limited</td>\n",
       "      <td>2025-05-02</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.020903</td>\n",
       "      <td>0.021618</td>\n",
       "      <td>1.588385</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>WIPRO.NS</td>\n",
       "      <td>Wipro Limited</td>\n",
       "      <td>2022-03-25</td>\n",
       "      <td>0.017254</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.082515</td>\n",
       "      <td>0.019121</td>\n",
       "      <td>0.951547</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>WIPRO.NS</td>\n",
       "      <td>Wipro Limited</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>0.005296</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>-0.000391</td>\n",
       "      <td>0.011284</td>\n",
       "      <td>1.042780</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>WIPRO.NS</td>\n",
       "      <td>Wipro Limited</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>0.004650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070166</td>\n",
       "      <td>0.019029</td>\n",
       "      <td>1.289916</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>WIPRO.NS</td>\n",
       "      <td>Wipro Limited</td>\n",
       "      <td>2025-01-17</td>\n",
       "      <td>0.021661</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-0.079417</td>\n",
       "      <td>0.014270</td>\n",
       "      <td>1.184603</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>WIPRO.NS</td>\n",
       "      <td>Wipro Limited</td>\n",
       "      <td>2025-07-17</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.006075</td>\n",
       "      <td>0.011021</td>\n",
       "      <td>1.057602</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>368 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Ticker                    Company Event_Date  Dividend_Yield  \\\n",
       "0    ADANIENT.NS  Adani Enterprises Limited 2021-05-05        0.000794   \n",
       "1    ADANIENT.NS  Adani Enterprises Limited 2022-05-04        0.000428   \n",
       "2    ADANIENT.NS  Adani Enterprises Limited 2023-05-04        0.000653   \n",
       "3    ADANIENT.NS  Adani Enterprises Limited 2024-05-02        0.000426   \n",
       "4    ADANIENT.NS  Adani Enterprises Limited 2025-05-02        0.000565   \n",
       "..           ...                        ...        ...             ...   \n",
       "363     WIPRO.NS              Wipro Limited 2022-03-25        0.017254   \n",
       "364     WIPRO.NS              Wipro Limited 2023-01-13        0.005296   \n",
       "365     WIPRO.NS              Wipro Limited 2024-01-12        0.004650   \n",
       "366     WIPRO.NS              Wipro Limited 2025-01-17        0.021661   \n",
       "367     WIPRO.NS              Wipro Limited 2025-07-17        0.019400   \n",
       "\n",
       "     Dividend_Change_Pct  Momentum_20d  Volatility_30d      Beta  \\\n",
       "0               0.000000      0.147261        0.043471  1.326688   \n",
       "1               0.000000      0.154828        0.020791  1.501592   \n",
       "2               0.200000      0.058887        0.028315  3.458845   \n",
       "3               0.083333     -0.043985        0.016967  1.732479   \n",
       "4               0.000000     -0.020903        0.021618  1.588385   \n",
       "..                   ...           ...             ...       ...   \n",
       "363             4.000000      0.082515        0.019121  0.951547   \n",
       "364            -0.800000     -0.000391        0.011284  1.042780   \n",
       "365             0.000000      0.070166        0.019029  1.289916   \n",
       "366             5.000000     -0.079417        0.014270  1.184603   \n",
       "367            -0.166667      0.006075        0.011021  1.057602   \n",
       "\n",
       "     DivType_Interim  \n",
       "0              False  \n",
       "1              False  \n",
       "2              False  \n",
       "3              False  \n",
       "4              False  \n",
       "..               ...  \n",
       "363             True  \n",
       "364             True  \n",
       "365             True  \n",
       "366             True  \n",
       "367             True  \n",
       "\n",
       "[368 rows x 9 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f41a31fb-05c9-4e86-8dc0-9a6d02616fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dividend_Yield         0\n",
      "Dividend_Change_Pct    0\n",
      "Momentum_20d           0\n",
      "Volatility_30d         0\n",
      "Beta                   0\n",
      "DivType_Interim        0\n",
      "dtype: int64\n",
      "Empty DataFrame\n",
      "Columns: [Ticker, Company, Event_Date, Dividend_Yield, Dividend_Change_Pct, Momentum_20d, Volatility_30d, Beta, DivType_Interim]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Count NaNs per feature\n",
    "print(feat_df[features].isna().sum())\n",
    "\n",
    "# Inspect rows with NaNs\n",
    "nan_rows = feat_df[feat_df[features].isna().any(axis=1)]\n",
    "print(nan_rows.head(10))   # show first few problematic rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef0e551-c794-4f63-be6b-e66501f63acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5fa0071-c94c-411b-abd8-2dd3f07477ee",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251e3970-62bf-4c24-89ec-2c7756db5d0c",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "51d7fac4-e0a9-4555-94a4-ac8d8e8726e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows merged (nonzero CAR): 368\n",
      "Test Accuracy: 0.613\n",
      "\n",
      "Confusion Matrix (rows=true, cols=pred; order: [ +1 , -1 ])\n",
      "[[31 21]\n",
      " [22 37]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.638     0.627     0.632        59\n",
      "           1      0.585     0.596     0.590        52\n",
      "\n",
      "    accuracy                          0.613       111\n",
      "   macro avg      0.611     0.612     0.611       111\n",
      "weighted avg      0.613     0.613     0.613       111\n",
      "\n",
      "\n",
      "Logistic coefficients (standardized):\n",
      "DivType_Interim        0.326043\n",
      "Dividend_Change_Pct    0.164113\n",
      "Momentum_20d           0.142726\n",
      "Dividend_Yield         0.101547\n",
      "Beta                   0.077771\n",
      "Volatility_30d         0.056029\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ==== 0) Imports ====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# ==== 1) Load data ====\n",
    "features_df = pd.read_csv(\"Features_Dataset.csv\")\n",
    "car_df      = pd.read_csv(\"CAR_by_event.csv\")\n",
    "\n",
    "# Ensure dates are datetime\n",
    "features_df[\"Event_Date\"] = pd.to_datetime(features_df[\"Event_Date\"], errors=\"coerce\")\n",
    "car_df[\"event_date\"]      = pd.to_datetime(car_df[\"event_date\"], errors=\"coerce\")\n",
    "\n",
    "# >>> FIX: align column names so merge keys match <<<\n",
    "car_df = car_df.rename(columns={\"event_date\": \"Event_Date\"})\n",
    "\n",
    "# ==== 2) Merge features + CAR on Ticker & Event_Date ====\n",
    "ml = features_df.merge(\n",
    "    car_df[[\"Ticker\", \"Event_Date\", \"CAR\"]],\n",
    "    on=[\"Ticker\", \"Event_Date\"],\n",
    "    how=\"inner\"\n",
    ").copy()\n",
    "\n",
    "# Drop missing/zero CAR (optional to drop zeros)\n",
    "ml = ml.dropna(subset=[\"CAR\"])\n",
    "ml = ml[ml[\"CAR\"] != 0]\n",
    "\n",
    "# Label: +1 if CAR>0 else -1\n",
    "ml[\"CAR_sign\"] = np.where(ml[\"CAR\"] > 0, 1, -1).astype(int)\n",
    "\n",
    "# Ensure DivType_Interim is numeric 0/1\n",
    "if ml[\"DivType_Interim\"].dtype == bool:\n",
    "    ml[\"DivType_Interim\"] = ml[\"DivType_Interim\"].astype(int)\n",
    "elif ml[\"DivType_Interim\"].dtype == object:\n",
    "    ml[\"DivType_Interim\"] = ml[\"DivType_Interim\"].map({True:1, False:0, \"True\":1, \"False\":0}).fillna(ml[\"DivType_Interim\"]).astype(float)\n",
    "\n",
    "# ==== 3) Features (X) and Target (y) ====\n",
    "features = [\"Dividend_Yield\",\"Dividend_Change_Pct\",\"Momentum_20d\",\"Volatility_30d\",\"Beta\",\"DivType_Interim\"]\n",
    "X = ml[features].copy()\n",
    "y = ml[\"CAR_sign\"]\n",
    "\n",
    "# ==== 4) Train/Test split ====\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\n",
    "\n",
    "# ==== 5) Pipeline: Impute → Scale → Logistic Regression ====\n",
    "pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\", solver=\"lbfgs\"))\n",
    "])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# ==== 6) Evaluation ====\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(f\"Rows merged (nonzero CAR): {len(ml)}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(\"\\nConfusion Matrix (rows=true, cols=pred; order: [ +1 , -1 ])\")\n",
    "print(confusion_matrix(y_test, y_pred, labels=[1,-1]))\n",
    "print(\"\\nClassification Report\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "# Coefficients (on standardized features)\n",
    "coef = pd.Series(pipe.named_steps[\"clf\"].coef_.ravel(), index=features)\n",
    "print(\"\\nLogistic coefficients (standardized):\")\n",
    "print(coef.sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c654f06-73a2-4dfb-a476-215c12d73244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8d50863-8ee8-4d18-88d7-3538944c460c",
   "metadata": {},
   "source": [
    "### Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "232020b8-d3bf-4c51-b807-72e4d4c93f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows merged (nonzero CAR): 368\n",
      "Random Forest Test Accuracy: 0.514\n",
      "\n",
      "Confusion Matrix (rows=true, cols=pred; order: [ +1 , -1 ])\n",
      "[[25 27]\n",
      " [27 32]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.542     0.542     0.542        59\n",
      "           1      0.481     0.481     0.481        52\n",
      "\n",
      "    accuracy                          0.514       111\n",
      "   macro avg      0.512     0.512     0.512       111\n",
      "weighted avg      0.514     0.514     0.514       111\n",
      "\n",
      "\n",
      "5-fold CV Accuracy: mean=0.533, std=0.046\n",
      "CV fold scores: [0.608 0.473 0.5   0.548 0.534]\n",
      "\n",
      "Random Forest feature importances:\n",
      "Dividend_Yield         0.215651\n",
      "Momentum_20d           0.205141\n",
      "Volatility_30d         0.202286\n",
      "Beta                   0.187830\n",
      "Dividend_Change_Pct    0.144594\n",
      "DivType_Interim        0.044497\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ==== 0) Imports ====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# ==== 1) Load & merge (same keys as before) ====\n",
    "features_df = pd.read_csv(\"Features_Dataset.csv\")\n",
    "car_df      = pd.read_csv(\"CAR_by_event.csv\")\n",
    "\n",
    "# Dates to datetime\n",
    "features_df[\"Event_Date\"] = pd.to_datetime(features_df[\"Event_Date\"], errors=\"coerce\")\n",
    "car_df[\"event_date\"]      = pd.to_datetime(car_df[\"event_date\"], errors=\"coerce\")\n",
    "\n",
    "# Align column names\n",
    "car_df = car_df.rename(columns={\"event_date\": \"Event_Date\"})\n",
    "\n",
    "# Merge features + CAR\n",
    "ml = features_df.merge(\n",
    "    car_df[[\"Ticker\", \"Event_Date\", \"CAR\"]],\n",
    "    on=[\"Ticker\", \"Event_Date\"],\n",
    "    how=\"inner\"\n",
    ").copy()\n",
    "\n",
    "# Keep valid, nonzero CAR and create label\n",
    "ml = ml.dropna(subset=[\"CAR\"])\n",
    "ml = ml[ml[\"CAR\"] != 0]\n",
    "ml[\"CAR_sign\"] = np.where(ml[\"CAR\"] > 0, 1, -1).astype(int)\n",
    "\n",
    "# Ensure dummy is numeric\n",
    "if ml[\"DivType_Interim\"].dtype == bool:\n",
    "    ml[\"DivType_Interim\"] = ml[\"DivType_Interim\"].astype(int)\n",
    "\n",
    "# ==== 2) Features & target ====\n",
    "features = [\"Dividend_Yield\",\"Dividend_Change_Pct\",\"Momentum_20d\",\"Volatility_30d\",\"Beta\",\"DivType_Interim\"]\n",
    "X = ml[features].copy()\n",
    "y = ml[\"CAR_sign\"]\n",
    "\n",
    "# ==== 3) Train/Test split ====\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ==== 4) Pipeline: Impute → RandomForest ====\n",
    "rf_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"rf\", RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=2,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "\n",
    "# ==== 5) Test evaluation ====\n",
    "y_pred = rf_pipe.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "cm  = confusion_matrix(y_test, y_pred, labels=[1,-1])\n",
    "\n",
    "print(f\"Rows merged (nonzero CAR): {len(ml)}\")\n",
    "print(f\"Random Forest Test Accuracy: {acc:.3f}\")\n",
    "print(\"\\nConfusion Matrix (rows=true, cols=pred; order: [ +1 , -1 ])\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "# ==== 6) 5-fold CV accuracy ====\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(rf_pipe, X, y, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
    "print(f\"\\n5-fold CV Accuracy: mean={cv_scores.mean():.3f}, std={cv_scores.std():.3f}\")\n",
    "print(\"CV fold scores:\", np.round(cv_scores, 3))\n",
    "\n",
    "# ==== 7) Feature importances ====\n",
    "# Pull trained RF out of the pipeline\n",
    "rf = rf_pipe.named_steps[\"rf\"]\n",
    "importances = pd.Series(rf.feature_importances_, index=features).sort_values(ascending=False)\n",
    "print(\"\\nRandom Forest feature importances:\")\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80397577-36cb-4a6c-bb33-915a513fdad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315df15c-41d0-499f-aef5-f8b07d05a0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
